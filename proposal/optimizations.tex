Now that I have a general compilation strategy, I need to apply optimizations.
As previously mentioned, there are many optimizations to choose from.

The theory of compiler optimizations is nearly half a century old.
Most commonly applied optimizations today originated back in 1969 \cite{optminzation_allen}.
In 1972 Allen et al. cataloged several optimizations, and showed how to use data flow analysis for optimization \cite{dataflow_allen}.
At this point all optimizations were local to a given block of code.
In 1973 Kindall showed how to do global program optimization using dataflow analysis \cite{dataflow_kildall}.
Alpern et al. developed the SSA intermediate representation where variables are never reassigned \cite{ssa_alpern}.
Wegman et al. showed several optimizations that took advantage of SSA \cite{ssa_Wegman}.
Stolts et al. used induction analysis to further optimize program \cite{ssa_wolfe}

\subsection{Problems With Traditional Optimizations}
While these optimizations are all useful, they may not be applicable to Curry.
As an example, one very prevalent optimization from functional programming is function inlining.
This is where we take function call and, after appropriately renaming variables,
replace the call with the function body.
The reduces the number of function calls we need to make, and gives us more code to make other optimizations.
This seems like a good candidate.
Unfortunately, this doesn't work well with non-determinism.
Suppose we have the code:
\begin{verbatim}
xor True True   = True
xor True False  = False
xor False True  = False
xor False False = True

xorSelf x = xor x x

xorSelf (True ? False)
\end{verbatim}
It should be the case that \texttt{xorSelf} will always return \texttt{False},
but if we inline the line \texttt{xorSelf (True ? False)} we will be left with \texttt{xor (True ? False) (True ? False)}.
This expression can now evaluate to \texttt{True}.
This violates the call-time choice semantics of Curry \cite{LopezFraguas14TPLP}.
Therefore, I cannot use simple textual replacement to implement inlining or $\beta$-reduction as in Haskell or ML.
This is another point in favor of using a graph-based IR.

We run into further problems with traditional optimizations.
The biggest problem is that they may not be effective.
Constant folding is a useful optimization for programs with a large amount of arithmetic,
but it may not be as effective in Curry.
I should be careful to avoid implementing optimizations that will not be effective.

To this end I am interested in optimizations that are going to be effective for Curry.
One major problem with Curry performance is memory usage.
As we have seen already, many Curry problems require much more space than they should.
This, in turn, prompts Curry programs to spend a lot of their running time creating and destroying
small pieces of memory.
One way to improve Curry performance is to reduce the number of nodes that are created and destroyed.
To that end, I've picked three optimizations to implement: unboxing, deforestation, and shortcutting.

\subsection{Unboxing}

Boxing refers to abstracting primitive data types in order to make a uniform representation of data.
This is a very common process in programming languages.  It makes development of the compiler easier,
and it reduces hard-to-find errors.
This is more important in lazy languages where a variable or parameter may be represented by unevaluated code
instead of a concrete value.
However, boxing has a major performance penalty.
All arithmetic with boxed values must first dereference the values.
In order to alleviate this performance penalty we would like to unbox values.
Unboxing is the process of replacing boxed literals with literal values.
Launchbury et al. showed that unboxing can be implemented in a lazy functional language \cite{unboxing}.
Hall et al. showed that unboxing can be implemented using partial evaluation \cite{unboxing_peval}.
It remains to be shown that unboxing can be implemented with non-deterministic values.
This seems reasonable, since non-deterministic values can behave like unevaluated expressions.

\subsection{Deforestation}
Deforestation is an optimization technique that originated with functional programming \cite{deforestation_wadler}.
The idea is that there are many functional programs that produce intermediate data structures, and immediately consume them.
Deforestation will rewrite these functions so that they do not create these data structures.
Wadler showed the initial deforestation transformation \cite{deforestation_wadler}.
While this transformation removed all intermediate structures, it was only applicable to a limited set of programs.
Launchbury et al. showed how this transformation could simplified and expanded to a wider class of programs \cite{shortcut_deforestation}.
Unfortunately this transformation isn't guaranteed to remove every intermediate stricture.
Currently, the standard deforestation technique is known as stream fusion.
This has been shown to be more effective than shortcut deforestation, 
but is requires re-writing much of the standard library \cite{stream}.
While the implementation of deforestation should be straightforward,
it remains to be shown the deforestation is still valid for non-deterministic expressions.

To see how deforestation works, consider the code for \texttt{all}:
\begin{verbatim}
all p xs = and (map p xs)
\end{verbatim}
While this code is correct, it's not optimal.
If we start with the list \texttt{[x0, x1, ... xn]}, then \texttt{map p} will produce the intermediate list
\texttt{[p x0, p x1, ... p xn]}.
Then finally we fold all of those values with all.

We could instead write the code:
\begin{verbatim}
all p xs = h xs
 where h [] = True
       h (y:ys) = p y && h ys
\end{verbatim}
This is less obviously correct, but it is more efficient.  We won't create the intermediate list.
We would like the compiler to automatically transform the first version into the second version.
We can accomplish this by recognizing that \texttt{and} and \texttt{map} can be implemented using \texttt{foldr} and \texttt{build}
respectively.

\begin{verbatim}
and = foldr (&&) True
map f xs = build (\c n -> foldr (\a b -> c (f a) b) n xs)
\end{verbatim}

Then we can apply the rule \texttt{foldr f z (build g) = g f z}.  This will produce the code
\begin{verbatim}
all p xs = foldr (&&) True (build (\c u -> foldr (\a b -> c (p a) b) n xs))
         = (\c n -> foldr (\a b -> c (p a) b) n xs) (&&) True 
         = foldr (\a b -> (p a) && b) True xs
\end{verbatim}
Substituting the definition for \texttt{foldr} we see that \texttt{all} reduces to:
\begin{verbatim}
all p xs = h xs
 where h [] = True
       h (y:ys) = (p a) && h ys
\end{verbatim}
This is the code we wanted originally.

\subsection{Shortcutting}
Shortcutting is a relatively recent idea for optimizing functional logic programs \cite{shortcutting}.
It is similar to the idea of deforestation, in that we are avoiding the construction of intermediate data structures.
However, the goal of shortcutting is to avoid the construction of intermediate function nodes.
Shortcutting has been shown to be effective in theory, but has yet to be implemented in a full compiler.

The idea is straightforward, however is it dependent on the compilation strategy for Curry.
When a function in Curry is compiled, code is generated to reduce that function symbol to head normal form.
Usually this code is uniform, and we represent it with the function $\H$.

\noindent
As an example:\\
$\H(length([])) = 0$\\
$\H(length(x:xs)) = \H(+(1,length(xs)))$\\
$\H(length(xs)) = \H(length(\H(xs)))$\\
$\H([]++ys) = \H(ys)$\\
$\H((x:xs)++ys) = x : (xs++ys)$\\
$\H(xs++ys) = \H(\H(xs) ++ ys)$\\

It's clear that we're creating more nodes then we need to.
For example, in the second rule for $\H(length)$ we create the node $+$ only to immediately evaluate it.
It would be beneficial if we could just execute the code for $+$ directly.
We accomplish this in two steps.
First we generate a separate reduction function for each type of node,
and second we allow a node to call other reduction functions.
The effect of this is that we don't need to construct nodes that we will immediately reduce.
Instead, we just call their reduction function.

For example, the compiled code for length would become:\\
$\Hf{length}([]) = 0$\\
$\Hf{length}(x:xs) = \Hf{+}(1, xs)$\\
$\Hf{length}(xs) = \H_{length}(\H(xs))$\\

The justification for this transformation is given in \cite{shortcutting}.
This transformation has been shown to speed up programs significantly over the naive version.
I also expect this optimization to work well with other optimizations.

It should be mentioned that other lazy functional languages can get an effect similar to this,
but it requires either inlining the function call,
or separate rules for compiling built in functions \cite{functional_PeytonJones}.
The shortcutting approach is conceptually simpler, and more general.
However, it comes at the cost of the size of the generated code.

All three of these optimizations are focused on saving memory.
While there are many other possible optimizations, I believe that these will be effective in a real Curry compiler.

