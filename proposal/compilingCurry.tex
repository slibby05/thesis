
\subsection{Curry Evaluation Model}

Currently, There are three compilers for Curry:
Pakcs, which compiles Curry into Prolog;
Kics2, which compiles Curry into Haskell;
and Sprite, which compiles Curry into LLVM code.
While these compilers all compile to different languages,
the more important difference is how they handle non-determinism.
In this section I'll take an in-depth look at these compilers,
and use these ideas to propose an adequate compilation strategy
to demonstrate optimizations.

\subsection{Pakcs}

The Pakcs compiler is built on the foundations of Prolog.
Not only does Pakcs compile Curry to Prolog,
but it does so in such a way that all of the non-standard features 
of Curry are handled by Prolog.
For example, free variables in Curry are translated to free variables in Prolog,
and the choice operator in Curry is translated into parallel clauses in Prolog.
The immediate consequence of this is that all non-determinism is done through backtracking.
While this strategy is efficient, it is not complete.
There are Curry programs that could produce an answer, but will fail to do so in Pakcs.

There are several advantages to translating Curry into Prolog.
One advantage is that the translation is very simple.
A compiler from FlatCurry to Prolog can be written over the course of a week.
Another advantage is the entirely reasonable assumption that the Prolog compiler
will do a better job of implementing non-determinism and free variables than a Curry compiler would.
The basis for this assumption is that more research and work has gone into the Prolog compiler.
Furthermore, any improvements of the efficiency of Prolog directly correlate to improvements
in the efficiency of Curry.
While these benefits are nice, we have already discussed the drawbacks of compiling to Prolog.

I will demonstrate the translation from Curry to Prolog using the \texttt{sum} example from before.

\begin{verbatim}
sum [] = 0
sum (x:xs) = x + xs
\end{verbatim}

We can see a simplified version of the code generated by Pakcs.
For simplicity I am removing code related to residuation \cite{Pakcs}.
While this is necessary for the Pakcs compiler, it is not used in modern Curry compilers.

\begin{verbatim}
sum(Arg,Result,Start,End) :- hnf(Arg, HArg, Start, Mid),
                             sum_1(HArg, Result, Mid, End).

sum_1([],      0,       End,   End).
sum_1([X|XS],  Result,  Start, End) :- !, hnf(+(X, sum(XS)),Result,Start,End).
sum_1(fail(F), fail(F), End,   End) :- nonvar(F).
\end{verbatim}

Those familiar with prolog may find this a little odd.
Why are we calling a mysterious \texttt{hnf} function to compute the value of \texttt{X + sum(XS)}?
In fact, why do we have two separate \texttt{sum} predicates at all?
Those familiar with functional interpreters may have already guessed where this is headed.
Pakcs doesn't compile Curry programs, it generates rules that interpret Curry expressions.
There are two rules generated, \texttt{nf} and \texttt{hnf}, which are responsible for reducing expressions
to normal form and head normal form respectively.

The \texttt{nf} rule is actually static across all programs.  It computes the head normal form of the argument,
and computes the normal form of all of the argument's children.
The \texttt{hnf} rule must be generated for every new program.
It is essentially a giant case discriminator for every possible function symbol.
As an example:

\begin{verbatim}
hnf(sum(XS), Result, Start, End) :- sum(XS,Result,Start,End).
hnf(foldl(OP,Z,XS), Result, Start End) :- foldl(OP,Z,XS, Result, Start, End).
hnf(++(XS,YS), Result, Start End) :- ++(XS, YS, Result, Start, End).
hnf(+(X,Y), Result, Start End) :- +(X, Y, Result, Start, End).
\end{verbatim}

Here each function is compiled in a manner similar to \texttt{sum}.
There are a few inherent inefficiencies in this style of compilation.
First, we construct several nodes that we don't need.
For example, if we evaluate the expression \texttt{sum [2]} we will produce the intermediate result.
\texttt{hnf(+(2, sum([])), R, S, E)}.
There is no reason to construct the \texttt + node.
Second, every expression that does not directly produce a value makes a call to the \texttt{hnf}
predicate.
This call to \texttt{hnf} is inherently slow, due to the fact that it contains a giant case discriminator.
The \texttt{hnf} predicate must contain a case for every function defined in the program, and any library that it imports.
This may result in hundreds or thousands of different cases.  Furthermore we call the \texttt{hnf} predicate many times.

\subsection{Kics2}

The Kics2 compiler takes a different approach, and compiles Curry to Haskell.
The hope is that deterministic functions can be compiled to Haskell code,
and therefore take advantage of GHC's many optimizations.
Unfortunately, as we have already seen, this doesn't quite work out.

Again, I'll demonstrate Kics2's compilation strategy by example.
Consider the sum function again.
I will simplify the generated code considerably for the sake of readability.
We can generate the following Haskell code:

\begin{verbatim}
sum :: OP_List C_Int -> Cover -> ConstStore -> C_Int
sum x1 cd cs = case x1 of
  List -> C_Int 0#
  Cons x2 x3 -> c_plus x2 (sum x3 cd cs) cd cs
  Choice_List d i l r -> narrow d i (sum l cd cs) (sum r cd cs)
  Fail_List d info -> failCons d (traceFail "sum" [show x1] info)

\end{verbatim}

Here we implement \texttt{sum} as a case statement.
Every constructor in Curry is augmented with two possibilities, \texttt{Choice} and \texttt{Fail},
so every function must check those two cases.
If the constructor is the empty list, then we return 0 as normal.
If the constructor is not empty, then we call the \texttt{c\_Plus} function defined in Kics2.
However, if we encounter a choice, we cannot proceed.
In this case we move the choice up one level in our expression, and compute each choice independently.
This is called pull-tabbing.

% There are two important things to note in this compilation scheme.
% First, since all primitive values must have their own, Curry specific, constructors,
% there is no way to convert them to primitive Haskell types.
% This limits what the GHC optimizer can accomplish.
% Second, we are again rewriting our expressions instead of just computing values.
% We rewrite the \texttt{sum [1,2]} to \texttt{1 + sum [2]}




\begin{itemize}
  \item Sprite: complete pull-tabbing
\end{itemize}
\subsection{Graph Rewriting vs Graph Reduction}
This is an example of the two different ideas of compiling functional programs.
Pakcs compiles all of the Curry functions into rewrite rules.
Expressions are rewritten until they reach a normal form.
The alternative to graph rewriting is graph reduction.
The idea is that the graph knows how to reduce itself to a normal form.
This is the approach commonly taken by functional language compilers.
Each node in the graph contains a pointer to code that will compute a value in head normal form for that node.
These nodes with code pointers are commonly known as closures.

While there is nothing showing that graph rewriting is inherently slower than graph reduction,
the common belief among the function community is that they are.
There is a possibility that graph rewriting may be competitive with graph reduction
with aggressive optimizations, but that is not the focus of this research.

\subsection{Compilation}
In order to optimize Curry, we need to have a well founded compilation model.
While we have looked at a few compilation models, I would like to present a new one.
This model has two main goals: simplicity, and efficiency.
I do not claim that this is the best model for Curry, but it is easy to understand, and reasonably efficient.

First we need to answer some fundamental questions about this model.
The first question to ask how should we represent the abstractions in Curry.
We have two abstractions in Curry that come from logic programming:
non-determinism and free variables.
While I could handle these independently, it is well known that free variables can be converted into
non-determinism and vice versa.
For example, in a language with only free variables we can construct the choice operator as follows:
\begin{verbatim}
(?) :: a -> a -> a
x ? y = if b then x else y
 where b free
\end{verbatim}
Furthermore, if we only have non-determinism, then we can construct values using a generator for each type:
\begin{verbatim}
freeBool :: Bool
freeBool = True ? False
\end{verbatim}
The expression \texttt{b where b free} could be translated to \texttt{freeBool} by the compiler.
I have no strong reason to prefer one of these representations over the other.
In fact, both of these representations have been proposed \cite{andrew, kics2}.
I plan to translate free variables to non-determinism for two reasons;
this is the currently the standard representation of Curry programs, and representing free variables explicitly
will likely require some clever trick like variant heaps \cite{andrew}.
This would complicate the compiler.

The second question to ask is how to represent non-determinism.
I believe that backtracking will be the best solution.
There are many reasons for this decision, but the strongest is that backtracking
is both simpler and more efficient than any other strategy for evaluating Curry programs.
This will give a realistic view of the efficiency of these optimizations.

There are some objections to using a backtracking evaluation strategy.
The most prominent is that backtracking isn't complete.
This is true, some computations will not find a result;
however, the purpose of this compiler is to demonstrate the improvement gained in optimizations.
If the program doesn't finish executing, then it could not be compared to a program that finished.


\subsection{compiler pipeline}
Since I am demonstrating the value of optimizations, I can reuse existing pieces of Curry compilers.
Currently, every Curry compiler produces an intermediate representation known as FlatCurry.
A FlatCurry program is equivalent to a Curry program, except that all of the functions have been transformed into
definitional trees.  The syntax is described in Figure \ref{flatcurry}

While FlatCurry is an intermediate representation, it is not well suited to optimization.
A large part of this dissertation will involve developing an IR that is sufficient for optimizing Curry programs.

\subsection{proposed curry compilation}
\begin{itemize}
  \item simple compilation
  \item backtracking
  \item intermediate representation: flatcurry
  \item intermediate representation: icurry?
  \item intermediate representation: A-curry?
\end{itemize}
