
\subsection{Curry Evaluation Model}

Currently, there are three compilers for Curry:
Pakcs, which compiles Curry into Prolog;
Kics2, which compiles Curry into Haskell;
and Sprite, which compiles Curry into LLVM code.
While these compilers all compile to different languages,
the more important difference is how they handle non-determinism.
In this section I'll take an in-depth look at these compilers,
and use these ideas to propose an adequate compilation strategy
to demonstrate optimizations.

\subsection{Pakcs}

The Pakcs compiler is built on the foundations of Prolog.
Not only does Pakcs compile Curry to Prolog,
but it does so in such a way that all of the non-standard features 
of Curry are handled by Prolog.
For example, free variables in Curry are translated to free variables in Prolog,
and the choice operator in Curry is translated into parallel clauses in Prolog.
The immediate consequence of this is that all non-determinism is done through backtracking.
While this strategy is efficient, it is not complete.
There are Curry programs that could produce an answer, but will fail to do so in Pakcs.

There are several advantages to translating Curry into Prolog.
One advantage is that the translation is very simple.
A compiler from FlatCurry to Prolog can be written over the course of a week.
Another advantage is the entirely reasonable assumption that the Prolog compiler
will do a better job of implementing non-determinism and free variables than a Curry compiler would.
The basis for this assumption is that more research and work has gone into the Prolog compiler.
Furthermore, any improvements of the efficiency of Prolog directly correlate to improvements
in the efficiency of Curry.
While these benefits are nice, I have already discussed the drawbacks of compiling to Prolog.

I will demonstrate the translation from Curry to Prolog using the \texttt{sum} example from before.

\begin{verbatim}
sum [] = 0
sum (x:xs) = x + xs
\end{verbatim}

Below is a simplified version of the code generated by Pakcs.
For simplicity I am removing code related to residuation \cite{Hanus17PAKCS}.
While this is necessary for the Pakcs compiler, it is not used in modern Curry compilers.

\begin{verbatim}
sum(Arg,Result,Start,End) :- hnf(Arg, HArg, Start, Mid),
                             sum_1(HArg, Result, Mid, End).

sum_1([],      0,       End,   End).
sum_1([X|XS],  Result,  Start, End) :- !, 
    hnf(+(X, sum_F(XS)),Result,Start,End).
sum_1(fail(F), fail(F), End,   End) :- nonvar(F).
\end{verbatim}

Those familiar with Prolog may find this a little odd.
Why are we calling a mysterious \texttt{hnf} predicate to compute the value of \texttt{X + sum(XS)}?
In fact, why do we have two separate \texttt{sum} predicates at all?
Those familiar with functional interpreters may have already guessed where this is headed.
Pakcs doesn't compile Curry programs, it generates rules that interpret Curry expressions.
There are two rules generated, \texttt{nf} and \texttt{hnf}, which are responsible for reducing expressions
to normal form and head normal form respectively.

The \texttt{nf} rule is actually static across all programs.  It computes the head normal form of the argument,
and computes the normal form of all of the argument's children.
The \texttt{hnf} rule must be generated for every new program.
It is essentially a giant case discriminator for every possible function symbol.
As an example:

\begin{verbatim}
hnf(sum_F(XS), Result, Start, End) :- 
  sum(XS,Result,Start,End).
hnf(foldl_F(OP,Z,XS), Result, Start End) :- 
  foldl(OP,Z,XS, Result, Start, End).
hnf(++_F(XS,YS), Result, Start End) :- 
  ++(XS, YS, Result, Start, End).
hnf(+_F(X,Y), Result, Start End) :- 
  +(X, Y, Result, Start, End).
\end{verbatim}

Here each function is compiled in a manner similar to \texttt{sum}.
There are a few inherent inefficiencies in this style of compilation.
First, we construct several nodes that we don't need.
For example, if we evaluate the expression \texttt{sum [2]} we will produce the intermediate result.
\texttt{hnf(+(2, sum\_F([])), R, S, E)}.
There is no reason to construct the \texttt + node.
Second, every expression that does not directly produce a value makes a call to the \texttt{hnf}
predicate.
This call to \texttt{hnf} is inherently slow, due to the fact that it contains a giant case discriminator.
In fact, it functions similarly to the giant switch statement in byte-code interpreters \cite{vmSwitch}.
The \texttt{hnf} predicate must contain a case for every function defined in the program, and any library that it imports.
This may result in hundreds or thousands of different cases.  Furthermore we call the \texttt{hnf} predicate many times.

\subsection{Kics2}

The Kics2 compiler takes a different approach, and compiles Curry to Haskell.
The hope is that deterministic functions can be compiled to Haskell code,
and therefore take advantage of GHC's many optimizations.
Unfortunately, as we have already seen, this doesn't quite work out.

Again, I'll demonstrate Kics2's compilation strategy by example.
Consider the sum function again.
I will simplify the generated code considerably for the sake of readability.
Sum will generate the following Haskell code:

\begin{verbatim}
sum :: OP_List C_Int -> Cover -> ConstStore -> C_Int
sum x1 cd cs = case x1 of
  List -> C_Int 0#
  Cons x2 x3 -> c_plus x2 (sum x3 cd cs) cd cs
  Choice d i l r -> narrow d i (sum l cd cs) (sum r cd cs)
  Fail_List d info -> failCons d (traceFail "sum" [show x1] info)

\end{verbatim}

Here we implement \texttt{sum} as a case statement.
Every constructor in Curry is augmented with two possibilities, \texttt{Choice} and \texttt{Fail},
so every function must check those two cases.
If the constructor is the empty list, then we return 0 as normal.
If the constructor is not empty, then we call the \texttt{c\_Plus} function defined in Kics2.
However, if we encounter a choice, we cannot proceed.
In this case we move the choice up one level in our expression, and compute each choice independently.
This is called pull-tabbing \cite{Antoy11ICLP}.
As an example the expression \texttt{sum ([1] ? [2])} will match the \texttt{Choice} case, and return 
\texttt{Choice cd i (sum l cd cs) (sum r cd cs)}.
Note that \texttt{(sum l cd cs)} and \texttt{(sum r cd cs)} will not be evaluated due to Haskell's laziness.



\subsection{Sprite}
The final Curry compiler is Sprite \cite{AntoyJost16LOPSTR}.
Sprite compiles Curry to LLVM assembly code, and uses a pull tabbing scheme.
The difference between Sprite and Kics2 is that Sprite uses the Fair Scheme \cite{fair_scheme} for executing choices.
The preliminary results for Sprite look promising. 
Unfortunately,  it isn't mature yet and thus would not be a good implementation target for my research.

\subsection{Graph Rewriting vs Graph Reduction}
These compilers show the two different ideas of compiling functional programs.
Pakcs compiles all of the Curry functions into rewrite rules.
Expressions are rewritten until they reach a normal form.
The alternative to graph rewriting is graph reduction.
The idea is that the graph knows how to reduce itself to a normal form.
This is the approach commonly taken by functional language compilers.
Each node in the graph contains a pointer to code that will compute a value in head normal form for that node.
These nodes with code pointers are commonly known as closures.

While there is nothing showing that graph rewriting is inherently slower than graph reduction,
the common belief among the functional community is that rewriting is slower.
There is a possibility that, with aggressive optimizations, graph rewriting may be competitive with graph reduction,
but that is not the focus of this research.

\subsection{Compilation}
In order to optimize Curry, I need to have a well founded compilation model.
While I have looked at a few compilation models, I would like to present a new one.
This model has two main goals: simplicity, and efficiency.
I do not claim that this is the best model for Curry, but it is easy to understand, and reasonably efficient.

First I need to answer some fundamental questions about this model.
The first question to ask is, ``How should I represent the abstractions in Curry?''
There are have two abstractions in Curry that come from logic programming:
non-determinism and free variables.
While I could handle these independently, it is well known that free variables can be converted into
non-determinism and vice versa.
For example, in a language with only free variables I can construct the choice operator as follows:
\begin{verbatim}
(?) :: a -> a -> a
x ? y = if b then x else y
 where b free
\end{verbatim}
Furthermore, if I only have non-determinism, then I can construct values using a generator for each type:
\begin{verbatim}
freeBool :: Bool
freeBool = True ? False
\end{verbatim}
The expression \texttt{b where b free} could be translated to \texttt{freeBool} by the compiler.
I have no strong reason to prefer one of these representations over the other.
In fact, both of these representations have been proposed \cite{curry_vm, Brassel2011PHD}.
I plan to translate free variables to non-determinism for two reasons:
this is currently the standard representation of Curry programs, and representing free variables explicitly
will likely require some clever trick such as variant heaps \cite{curry_vm}.
This would complicate the compiler.

The second question to ask is, ``How to represent non-determinism?''
I believe that backtracking will be the best solution.
There are many reasons for this decision, but the strongest is that backtracking
is both simpler and more efficient than any other strategy for evaluating Curry programs.
This will give a realistic view of the efficiency of these optimizations.

There are some objections to using a backtracking evaluation strategy.
The most prominent is that backtracking isn't complete.
This is true. Some computations will not find a result,
however the purpose of this compiler is to demonstrate the improvement gained in optimizations.
If the program doesn't finish executing, then it could not be compared to a program that finished.


\subsection{Compiler Pipeline}
Since I am demonstrating the value of optimizations, I can reuse existing pieces of Curry compilers.
Currently, every Curry compiler produces an intermediate representation known as FlatCurry.
A FlatCurry program represents a Curry program, after the transformation to definitional trees has been made.

While FlatCurry is an intermediate representation, it is not well suited to optimization.
A large part of this dissertation will involve developing an IR that is sufficient for optimizing Curry programs.
An appropriate IR should represent the basic operations to evaluate a program.
In imperative languages, IRs are similar to assembly code.
In functional languages, IRs are variants of lambda calculus, where the fundamental operation is function application.
In logic languages, the WAM uses replacement and choices as its fundamental operations.
While I might want to use a combination of lambda calculus and the WAM, this does not seem like the right abstraction.
Furthermore, the WAM is designed for a backtracking system, so this would not be a general solution.

Instead, I propose an IR based on graph rewriting.
This is not an entirely new idea \cite{graph_ir, dactl}, 
but, to my knowledge, no one has used an IR based on graph rewriting for optimization.
This IR would be a variation on the ICurry language \cite{AntoyJost16LOPSTR}.
Currently the IR is not able to incorporate all of the optimizations.
However, I believe that this IR is a good starting place for optimizing Curry.

